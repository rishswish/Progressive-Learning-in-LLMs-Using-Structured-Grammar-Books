{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDwxiBESP3lz"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kijEm_G5DTe"
      },
      "outputs": [],
      "source": [
        "!unzip lessons_final.zip -d lessons_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DupQZG-1ljFP"
      },
      "source": [
        "# Pretraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FjwfwG2e1H1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    T5Tokenizer, T5Config, T5ForConditionalGeneration,\n",
        "    Trainer, TrainingArguments, EarlyStoppingCallback,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€ Device â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€ Simple span-masking â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def simple_t5_mask(text):\n",
        "    words = text.strip().split()\n",
        "    if len(words) < 4:\n",
        "        return text, text\n",
        "    span_len = random.randint(1, min(3, len(words) - 1))\n",
        "    start = random.randint(0, len(words) - span_len)\n",
        "    masked = words[:start] + [\"<extra_id_0>\"] + words[start + span_len:]\n",
        "    return \" \".join(masked), \"<extra_id_0> \" + \" \".join(words[start:start + span_len])\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€ Preprocess â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def preprocess(example, tokenizer):\n",
        "    src = tokenizer(\n",
        "        example[\"input\"],\n",
        "        padding=\"max_length\", truncation=True, max_length=256\n",
        "    )\n",
        "    tgt = tokenizer(\n",
        "        example[\"output\"],\n",
        "        padding=\"max_length\", truncation=True, max_length=64\n",
        "    )\n",
        "    src[\"labels\"] = [t if t != tokenizer.pad_token_id else -100\n",
        "                     for t in tgt[\"input_ids\"]]\n",
        "    return src\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€ Prediction Cleaning â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def clean_prediction(raw, tokenizer):\n",
        "    return raw.replace(tokenizer.pad_token, \"\")\\\n",
        "              .replace(tokenizer.eos_token, \"\")\\\n",
        "              .strip()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€ Evaluation â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def evaluate(model, tokenizer, dataset):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for ex in dataset:\n",
        "        enc = tokenizer(\n",
        "            ex[\"input\"], return_tensors=\"pt\",\n",
        "            padding=True, truncation=True, max_length=256\n",
        "        ).to(device)\n",
        "        with torch.no_grad():\n",
        "            ids = model.generate(\n",
        "                input_ids=enc[\"input_ids\"],\n",
        "                attention_mask=enc[\"attention_mask\"],\n",
        "                max_new_tokens=20, do_sample=False\n",
        "            )\n",
        "        pred = clean_prediction(tokenizer.decode(ids[0], skip_special_tokens=False), tokenizer)\n",
        "        if pred == ex[\"output\"].strip():\n",
        "            correct += 1\n",
        "    acc = correct / len(dataset)\n",
        "    print(f\"âœ… Eval Accuracy: {correct}/{len(dataset)} = {acc:.2f}\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€ Training perâ€level â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def train_level(dataset, tokenizer, model, args, level):\n",
        "    print(f\"\\nğŸ” Training curriculum level {level}\")\n",
        "    tokenized = dataset.map(\n",
        "        lambda ex: preprocess(ex, tokenizer),\n",
        "        remove_columns=[\"input\", \"output\"]\n",
        "    )\n",
        "    split = tokenized.train_test_split(test_size=0.05, seed=42)\n",
        "    print(f\"{len(split['train'])} train | {len(split['test'])} val\")\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=split['train'],\n",
        "        eval_dataset=split['test'],\n",
        "        tokenizer=tokenizer,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        "        data_collator=DataCollatorWithPadding(tokenizer)\n",
        "    )\n",
        "    trainer.train()\n",
        "    del trainer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€ Setup tokenizer + **randomâ€init model** â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "# load the default T5â€base config, but do NOT load its weights:\n",
        "config = T5Config.from_pretrained(\"t5-base\")\n",
        "model  = T5ForConditionalGeneration(config).to(device)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./t5_scratch_ablation\",\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=10,\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "\n",
        "    # do evaluation & checkpointing every epoch\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    # automatically reload best checkpoint\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€ Build curriculum â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "pkl_folder = \"./lessons_final/lessons_final\"\n",
        "curriculum = []\n",
        "import re\n",
        "\n",
        "def extract_vol_lesson(fn):\n",
        "    m = re.match(r'v(\\d+)_l(\\d+)\\.pkl', fn)\n",
        "    return (int(m[1]), int(m[2])) if m else (999,999)\n",
        "\n",
        "files = sorted(\n",
        "    [f for f in os.listdir(pkl_folder) if f.endswith(\".pkl\")],\n",
        "    key=extract_vol_lesson\n",
        ")\n",
        "\n",
        "for fname in files:\n",
        "    with open(os.path.join(pkl_folder, fname), \"rb\") as f:\n",
        "        docs = pickle.load(f)\n",
        "    examples = []\n",
        "    for d in docs:\n",
        "        text = d.text if hasattr(d, \"text\") else str(d)\n",
        "        m, t = simple_t5_mask(text)\n",
        "        examples.append({\"input\": m, \"output\": t})\n",
        "    curriculum.append((fname, Dataset.from_list(examples)))\n",
        "    print(f\"âœ… Loaded {fname} ({len(examples)} examples)\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€ Run curriculum â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "for i, (lesson_name, ds) in enumerate(curriculum, start=1):\n",
        "    # if i == 1: break\n",
        "    print(f\"â–¶ï¸ Lesson {lesson_name}\")\n",
        "    train_level(ds, tokenizer, model, args, i)\n",
        "\n",
        "print(\"\\nğŸ§ª Final Test Generation:\")\n",
        "test_text = \"They are <extra_id_0> the car at the <extra_id_1>.\"\n",
        "print(\"INPUT :\", test_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SdXPNRlgqU8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwdZXFMdgqfr",
        "outputId": "5800034e-7a83-45e2-82a0-c9377fcd71be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('my_syntax_t5_model/tokenizer_config.json',\n",
              " 'my_syntax_t5_model/special_tokens_map.json',\n",
              " 'my_syntax_t5_model/spiece.model',\n",
              " 'my_syntax_t5_model/added_tokens.json')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "save_dir = \"my_syntax_t5_model\"\n",
        "\n",
        "model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zvhes4FLg0dj"
      },
      "outputs": [],
      "source": [
        "save_dir = \"my_syntax_t5_model\"\n",
        "\n",
        "# config = T5Config.from_pretrained(\"t5-base\")\n",
        "# model  = T5ForConditionalGeneration(config).to(device)\n",
        "model = T5ForConditionalGeneration.from_pretrained('my_syntax_t5_model').to(device)\n",
        "tokenizer = T5Tokenizer.from_pretrained(save_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tVsquofLgiUh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "from transformers import (\n",
        "    T5Tokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
        "\n",
        "# â”€â”€â”€ Device â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# â”€â”€â”€ Tokenizer & Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "# add 6 special labelâ€tokens\n",
        "label_names  = [\"ABBR\", \"ENTY\", \"DESC\", \"HUM\", \"LOC\", \"NUM\"]\n",
        "label_tokens = [f\"<LABEL_{i}>\" for i in range(len(label_names))]\n",
        "tokenizer.add_tokens(label_tokens)\n",
        "\n",
        "# load T5â€base (pretrained) for classification\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"my_syntax_t5_model\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# mappings: int â†’ token â†’ human label\n",
        "label_map     = {i: label_tokens[i] for i in range(len(label_tokens))}\n",
        "inv_label_map = {tok: label_names[i] for i, tok in label_map.items()}\n",
        "\n",
        "# â”€â”€â”€ Load Dataset & Preparsed .pkl â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "trec = load_dataset(\"CogComp/trec\")\n",
        "with open(\"trec_train_docs.pkl\", \"rb\") as f:\n",
        "    train_docs = pickle.load(f)\n",
        "with open(\"trec_test_docs.pkl\", \"rb\") as f:\n",
        "    test_docs  = pickle.load(f)\n",
        "\n",
        "train_exs = [\n",
        "    {\"text\": doc.text, \"coarse_label\": trec[\"train\"][i][\"coarse_label\"]}\n",
        "    for i, doc in enumerate(train_docs)\n",
        "]\n",
        "test_exs = [\n",
        "    {\"text\": doc.text, \"coarse_label\": trec[\"test\"][i][\"coarse_label\"]}\n",
        "    for i, doc in enumerate(test_docs)\n",
        "]\n",
        "\n",
        "train_ds = Dataset.from_list(train_exs)\n",
        "test_ds  = Dataset.from_list(test_exs)\n",
        "\n",
        "# â”€â”€â”€ Preprocessing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def preprocess_clf(examples):\n",
        "    inputs  = examples[\"text\"]\n",
        "    targets = [label_map[l] for l in examples[\"coarse_label\"]]\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=64,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # tokenize targets; we only want the *first* token as the label\n",
        "    lbl = tokenizer(\n",
        "        targets,\n",
        "        max_length=1,\n",
        "        padding=\"max_length\",\n",
        "        truncation=False\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    # build labels: first token is the label, rest is -100\n",
        "    model_inputs[\"labels\"] = [\n",
        "        [seq[0]] + [-100] * (len(seq) - 1) for seq in lbl\n",
        "    ]\n",
        "    return model_inputs\n",
        "\n",
        "train_tok = train_ds.map(\n",
        "    preprocess_clf,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\", \"coarse_label\"]\n",
        ")\n",
        "test_tok = test_ds.map(\n",
        "    preprocess_clf,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\", \"coarse_label\"]\n",
        ")\n",
        "\n",
        "train_tok.set_format(\"torch\")\n",
        "test_tok.set_format(\"torch\")\n",
        "\n",
        "# â”€â”€â”€ Torch Dataset Wrapper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class ClfDataset(TorchDataset):\n",
        "    def __init__(self, hf_ds): self.ds = hf_ds\n",
        "    def __len__(self): return len(self.ds)\n",
        "    def __getitem__(self, i):\n",
        "        ex = self.ds[i]\n",
        "        return {\n",
        "            \"input_ids\":      ex[\"input_ids\"],\n",
        "            \"attention_mask\": ex[\"attention_mask\"],\n",
        "            \"labels\":         ex[\"labels\"],\n",
        "        }\n",
        "\n",
        "train_torch = ClfDataset(train_tok)\n",
        "test_torch  = ClfDataset(test_tok)\n",
        "\n",
        "# â”€â”€â”€ Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "    preds = logits.argmax(-1)\n",
        "\n",
        "    # replace -100 with pad_token_id for decoding\n",
        "    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n",
        "\n",
        "    decoded_preds  = tokenizer.batch_decode(preds, skip_special_tokens=False)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=False)\n",
        "\n",
        "    y_true, y_pred = [], []\n",
        "    for p_str, l_str in zip(decoded_preds, decoded_labels):\n",
        "        p_tok = p_str.strip().split()[0] if p_str.strip() else \"\"\n",
        "        l_tok = l_str.strip().split()[0] if l_str.strip() else \"\"\n",
        "        y_pred.append(inv_label_map.get(p_tok, \"???\"))\n",
        "        y_true.append(inv_label_map.get(l_tok, \"???\"))\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"recall\":   recall_score(y_true, y_pred, average=\"macro\"),\n",
        "        \"f1\":       f1_score(y_true, y_pred, average=\"macro\"),\n",
        "    }\n",
        "\n",
        "# â”€â”€â”€ TrainingArguments & Trainer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./t5_trec_labeltok\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=1e-5,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_torch,\n",
        "    eval_dataset=test_torch,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# â”€â”€â”€ Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "trainer.train()\n",
        "trainer.evaluate()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
